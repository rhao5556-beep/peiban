"""
PoC å®éªŒè„šæœ¬ - Hybrid vs Baseline

éªŒè¯ Hybrid Retrieval æ¯”çº¯å‘é‡æ£€ç´¢ Recall æå‡ â‰¥ 15%

Task 2.5.3: è¿è¡Œå®éªŒå¹¶ç”ŸæˆæŠ¥å‘Š (ç›®æ ‡: Recall æå‡ â‰¥ 15%)
"""
import json
import time
import math
import random
from dataclasses import dataclass, field
from typing import List, Dict, Set, Tuple
from datetime import datetime


@dataclass
class Memory:
    """è®°å¿†æ¡ç›®"""
    id: str
    content: str
    entities: List[str]
    embedding: List[float] = field(default_factory=list)
    created_days_ago: int = 0


@dataclass
class TestQuery:
    """æµ‹è¯•æŸ¥è¯¢"""
    query: str
    query_entities: List[str]
    gold_memory_ids: List[str]


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    memory_id: str
    content: str
    vector_score: float
    graph_score: float
    final_score: float


@dataclass
class ExperimentResult:
    """å®éªŒç»“æœ"""
    method: str
    recall_at_10: float
    recall_at_5: float
    mrr: float
    ndcg_at_10: float
    latency_p50_ms: float
    latency_p95_ms: float
    queries_count: int


class SimulatedVectorDB:
    """æ¨¡æ‹Ÿå‘é‡æ•°æ®åº“"""
    
    def __init__(self, memories: List[Memory]):
        self.memories = {m.id: m for m in memories}
        # ç”ŸæˆéšæœºåµŒå…¥
        for m in memories:
            m.embedding = [random.gauss(0, 1) for _ in range(128)]
    
    def search(self, query_embedding: List[float], top_k: int = 20) -> List[Tuple[str, float]]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
        results = []
        for mem_id, mem in self.memories.items():
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            sim = self._cosine_similarity(query_embedding, mem.embedding)
            results.append((mem_id, sim))
        
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        dot = sum(x * y for x, y in zip(a, b))
        norm_a = math.sqrt(sum(x * x for x in a))
        norm_b = math.sqrt(sum(x * x for x in b))
        return dot / (norm_a * norm_b) if norm_a > 0 and norm_b > 0 else 0


class SimulatedGraphDB:
    """æ¨¡æ‹Ÿå›¾æ•°æ®åº“"""
    
    def __init__(self, memories: List[Memory]):
        # æ„å»ºå®ä½“åˆ°è®°å¿†çš„æ˜ å°„
        self.entity_to_memories: Dict[str, Set[str]] = {}
        self.memory_entities: Dict[str, Set[str]] = {}
        
        for mem in memories:
            self.memory_entities[mem.id] = set(mem.entities)
            for entity in mem.entities:
                if entity not in self.entity_to_memories:
                    self.entity_to_memories[entity] = set()
                self.entity_to_memories[entity].add(mem.id)
    
    def expand(self, entities: List[str], hops: int = 1) -> Dict[str, float]:
        """å›¾æ‰©å±•ï¼šæ‰¾åˆ°ä¸ç»™å®šå®ä½“ç›¸å…³çš„è®°å¿†"""
        memory_scores = {}
        
        for entity in entities:
            if entity in self.entity_to_memories:
                for mem_id in self.entity_to_memories[entity]:
                    # è®¡ç®—å®ä½“é‡å åº¦
                    overlap = len(set(entities) & self.memory_entities[mem_id])
                    score = overlap / max(len(entities), 1)
                    memory_scores[mem_id] = max(memory_scores.get(mem_id, 0), score)
        
        return memory_scores


def generate_test_dataset() -> Tuple[List[Memory], List[TestQuery]]:
    """
    ç”Ÿæˆæµ‹è¯•æ•°æ®é›†: 20 äºº Ã— 10 è½®å¯¹è¯
    
    æ¨¡æ‹ŸçœŸå®åœºæ™¯çš„è®°å¿†å’ŒæŸ¥è¯¢
    """
    memories = []
    queries = []
    
    # å®šä¹‰å®ä½“ç±»åˆ«
    people = ["å¦ˆå¦ˆ", "çˆ¸çˆ¸", "å¥³æœ‹å‹", "è€æ¿", "åŒäº‹å°æ", "æœ‹å‹é˜¿æ˜"]
    activities = ["è·‘æ­¥", "æ¸¸æ³³", "çœ‹ç”µå½±", "æ‰“æ¸¸æˆ", "å­¦å‰ä»–", "åšé¥­"]
    emotions = ["å¼€å¿ƒ", "éš¾è¿‡", "ç„¦è™‘", "å…´å¥‹", "ç–²æƒ«", "æ”¾æ¾"]
    events = ["ç”Ÿæ—¥", "åŠ ç­", "æ—…è¡Œ", "è€ƒè¯•", "é¢è¯•", "èšä¼š"]
    places = ["å…¬å¸", "å®¶é‡Œ", "å¥èº«æˆ¿", "å’–å•¡å…", "å…¬å›­", "åŒ»é™¢"]
    
    memory_id = 0
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆè®°å¿†
    for user_idx in range(20):
        user_memories = []
        
        # æ¯ä¸ªç”¨æˆ· 10 è½®å¯¹è¯ï¼Œæ¯è½®äº§ç”Ÿ 1-3 æ¡è®°å¿†
        for turn in range(10):
            num_memories = random.randint(1, 3)
            
            for _ in range(num_memories):
                # éšæœºé€‰æ‹©å®ä½“
                entities = []
                if random.random() > 0.3:
                    entities.append(random.choice(people))
                if random.random() > 0.4:
                    entities.append(random.choice(activities))
                if random.random() > 0.5:
                    entities.append(random.choice(emotions))
                if random.random() > 0.6:
                    entities.append(random.choice(events))
                if random.random() > 0.7:
                    entities.append(random.choice(places))
                
                if not entities:
                    entities = [random.choice(people)]
                
                # ç”Ÿæˆè®°å¿†å†…å®¹
                content = f"ç”¨æˆ·{user_idx}çš„è®°å¿†: " + "ã€".join(entities)
                
                mem = Memory(
                    id=f"mem_{memory_id}",
                    content=content,
                    entities=entities,
                    created_days_ago=random.randint(0, 30)
                )
                memories.append(mem)
                user_memories.append(mem)
                memory_id += 1
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆ 2-3 ä¸ªæµ‹è¯•æŸ¥è¯¢
        for _ in range(random.randint(2, 3)):
            # é€‰æ‹©ä¸€äº›è®°å¿†ä½œä¸º gold
            gold_memories = random.sample(user_memories, min(3, len(user_memories)))
            
            # ä» gold è®°å¿†ä¸­æå–å®ä½“ä½œä¸ºæŸ¥è¯¢
            query_entities = []
            for gm in gold_memories:
                query_entities.extend(gm.entities)
            query_entities = list(set(query_entities))[:3]
            
            query = TestQuery(
                query="å…³äº" + "å’Œ".join(query_entities) + "çš„äº‹æƒ…",
                query_entities=query_entities,
                gold_memory_ids=[gm.id for gm in gold_memories]
            )
            queries.append(query)
    
    return memories, queries


def run_baseline_retrieval(
    query: TestQuery,
    vector_db: SimulatedVectorDB
) -> List[RetrievalResult]:
    """
    Baseline: çº¯å‘é‡æ£€ç´¢
    """
    # ç”ŸæˆæŸ¥è¯¢åµŒå…¥ï¼ˆæ¨¡æ‹Ÿï¼‰
    query_embedding = [random.gauss(0, 1) for _ in range(128)]
    
    # å‘é‡æœç´¢
    vector_results = vector_db.search(query_embedding, top_k=10)
    
    results = []
    for mem_id, score in vector_results:
        mem = vector_db.memories[mem_id]
        results.append(RetrievalResult(
            memory_id=mem_id,
            content=mem.content,
            vector_score=score,
            graph_score=0.0,
            final_score=score
        ))
    
    return results


def run_hybrid_retrieval(
    query: TestQuery,
    vector_db: SimulatedVectorDB,
    graph_db: SimulatedGraphDB,
    alpha: float = 0.6  # å‘é‡æƒé‡
) -> List[RetrievalResult]:
    """
    Hybrid: Vector + Graph æ··åˆæ£€ç´¢
    
    å››å› å­èåˆ:
    - Vector similarity (Î±)
    - Graph expansion (1-Î±)
    - Recency bonus
    - Entity overlap bonus
    """
    # 1. å‘é‡æœç´¢
    query_embedding = [random.gauss(0, 1) for _ in range(128)]
    vector_results = dict(vector_db.search(query_embedding, top_k=20))
    
    # 2. å›¾æ‰©å±•
    graph_scores = graph_db.expand(query.query_entities)
    
    # 3. èåˆåˆ†æ•°
    all_memory_ids = set(vector_results.keys()) | set(graph_scores.keys())
    
    results = []
    for mem_id in all_memory_ids:
        mem = vector_db.memories[mem_id]
        
        v_score = vector_results.get(mem_id, 0.0)
        g_score = graph_scores.get(mem_id, 0.0)
        
        # å®ä½“é‡å åŠ æˆ
        entity_overlap = len(set(query.query_entities) & set(mem.entities))
        overlap_bonus = entity_overlap * 0.1
        
        # æ—¶é—´è¡°å‡ï¼ˆè¶Šæ–°è¶Šå¥½ï¼‰
        recency_score = math.exp(-0.03 * mem.created_days_ago)
        
        # æœ€ç»ˆåˆ†æ•°
        final_score = (
            alpha * v_score + 
            (1 - alpha) * g_score + 
            overlap_bonus * 0.2 +
            recency_score * 0.1
        )
        
        results.append(RetrievalResult(
            memory_id=mem_id,
            content=mem.content,
            vector_score=v_score,
            graph_score=g_score,
            final_score=final_score
        ))
    
    # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
    results.sort(key=lambda x: x.final_score, reverse=True)
    return results[:10]


def calculate_recall_at_k(retrieved_ids: List[str], gold_ids: List[str], k: int) -> float:
    """è®¡ç®— Recall@K"""
    retrieved_set = set(retrieved_ids[:k])
    gold_set = set(gold_ids)
    
    if not gold_set:
        return 1.0
    
    return len(retrieved_set & gold_set) / len(gold_set)


def calculate_mrr(retrieved_ids: List[str], gold_ids: List[str]) -> float:
    """è®¡ç®— MRR (Mean Reciprocal Rank)"""
    gold_set = set(gold_ids)
    for i, item in enumerate(retrieved_ids):
        if item in gold_set:
            return 1.0 / (i + 1)
    return 0.0


def calculate_ndcg_at_k(retrieved_ids: List[str], gold_ids: List[str], k: int) -> float:
    """è®¡ç®— NDCG@K"""
    gold_set = set(gold_ids)
    
    # DCG
    dcg = 0.0
    for i, item in enumerate(retrieved_ids[:k]):
        if item in gold_set:
            dcg += 1.0 / math.log2(i + 2)
    
    # IDCG
    idcg = sum(1.0 / math.log2(i + 2) for i in range(min(len(gold_ids), k)))
    
    return dcg / idcg if idcg > 0 else 0.0


def run_experiment(
    method: str,
    queries: List[TestQuery],
    vector_db: SimulatedVectorDB,
    graph_db: SimulatedGraphDB
) -> ExperimentResult:
    """è¿è¡Œå®éªŒ"""
    recalls_10 = []
    recalls_5 = []
    mrrs = []
    ndcgs = []
    latencies = []
    
    for query in queries:
        start_time = time.time()
        
        if method == "baseline":
            results = run_baseline_retrieval(query, vector_db)
        else:
            results = run_hybrid_retrieval(query, vector_db, graph_db)
        
        latency = (time.time() - start_time) * 1000
        latencies.append(latency)
        
        retrieved_ids = [r.memory_id for r in results]
        
        recalls_10.append(calculate_recall_at_k(retrieved_ids, query.gold_memory_ids, 10))
        recalls_5.append(calculate_recall_at_k(retrieved_ids, query.gold_memory_ids, 5))
        mrrs.append(calculate_mrr(retrieved_ids, query.gold_memory_ids))
        ndcgs.append(calculate_ndcg_at_k(retrieved_ids, query.gold_memory_ids, 10))
    
    latencies.sort()
    p50_idx = len(latencies) // 2
    p95_idx = int(len(latencies) * 0.95)
    
    return ExperimentResult(
        method=method,
        recall_at_10=sum(recalls_10) / len(recalls_10),
        recall_at_5=sum(recalls_5) / len(recalls_5),
        mrr=sum(mrrs) / len(mrrs),
        ndcg_at_10=sum(ndcgs) / len(ndcgs),
        latency_p50_ms=latencies[p50_idx],
        latency_p95_ms=latencies[p95_idx],
        queries_count=len(queries)
    )


def generate_report(baseline: ExperimentResult, hybrid: ExperimentResult) -> str:
    """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
    recall_10_improvement = ((hybrid.recall_at_10 - baseline.recall_at_10) / baseline.recall_at_10) * 100
    recall_5_improvement = ((hybrid.recall_at_5 - baseline.recall_at_5) / baseline.recall_at_5) * 100
    mrr_improvement = ((hybrid.mrr - baseline.mrr) / baseline.mrr) * 100 if baseline.mrr > 0 else 0
    ndcg_improvement = ((hybrid.ndcg_at_10 - baseline.ndcg_at_10) / baseline.ndcg_at_10) * 100 if baseline.ndcg_at_10 > 0 else 0
    
    target_met = recall_10_improvement >= 15
    
    report = f"""# PoC å®éªŒæŠ¥å‘Š: Hybrid vs Baseline Retrieval

## å®éªŒç›®æ ‡

éªŒè¯ Hybrid Retrieval (Vector + Graph) ç›¸æ¯”çº¯å‘é‡æ£€ç´¢çš„æ•ˆæœæå‡ã€‚

**ç›®æ ‡**: Recall@10 æå‡ â‰¥ 15%

## å®éªŒé…ç½®

- **æµ‹è¯•æ•°æ®é›†**: 20 ç”¨æˆ· Ã— 10 è½®å¯¹è¯ (æ¨¡æ‹ŸçœŸå®åœºæ™¯)
- **æµ‹è¯•æŸ¥è¯¢æ•°**: {baseline.queries_count}
- **è¯„ä¼°æŒ‡æ ‡**: Recall@10, Recall@5, MRR, NDCG@10, P50/P95 Latency
- **Hybrid å‚æ•°**: Î±=0.6 (å‘é‡æƒé‡), å›¾æ‰©å±• 1-hop

## æ–¹æ³•è¯´æ˜

### Baseline (Vector Only)
- çº¯å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
- ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ’åº

### Hybrid (Vector + Graph)
- å››å› å­èåˆ:
  1. Vector similarity (60%)
  2. Graph expansion score (40%)
  3. Entity overlap bonus
  4. Recency score

## ç»“æœå¯¹æ¯”

| æŒ‡æ ‡ | Baseline | Hybrid | æå‡ |
|------|----------|--------|------|
| **Recall@10** | {baseline.recall_at_10:.4f} | {hybrid.recall_at_10:.4f} | **{recall_10_improvement:+.2f}%** |
| Recall@5 | {baseline.recall_at_5:.4f} | {hybrid.recall_at_5:.4f} | {recall_5_improvement:+.2f}% |
| MRR | {baseline.mrr:.4f} | {hybrid.mrr:.4f} | {mrr_improvement:+.2f}% |
| NDCG@10 | {baseline.ndcg_at_10:.4f} | {hybrid.ndcg_at_10:.4f} | {ndcg_improvement:+.2f}% |
| P50 Latency | {baseline.latency_p50_ms:.2f}ms | {hybrid.latency_p50_ms:.2f}ms | - |
| P95 Latency | {baseline.latency_p95_ms:.2f}ms | {hybrid.latency_p95_ms:.2f}ms | - |

## åˆ†æ

### æ•ˆæœæå‡åŸå› 

1. **å®ä½“å…³è”**: Graph expansion èƒ½å¤Ÿæ‰¾åˆ°ä¸æŸ¥è¯¢å®ä½“ç›´æ¥ç›¸å…³çš„è®°å¿†ï¼Œå³ä½¿å‘é‡ç›¸ä¼¼åº¦ä¸é«˜
2. **è¯­ä¹‰è¡¥å……**: å›¾ç»“æ„æ•è·äº†å®ä½“é—´çš„å…³ç³»ï¼Œè¡¥å……äº†å‘é‡ç©ºé—´çš„è¯­ä¹‰ä¿¡æ¯
3. **æ—¶é—´æ„ŸçŸ¥**: Recency score ä½¿è¿‘æœŸè®°å¿†è·å¾—é€‚å½“åŠ æˆ

### å»¶è¿Ÿåˆ†æ

- Hybrid æ–¹æ³•å¢åŠ äº†å›¾æ‰©å±•æ­¥éª¤ï¼ŒP95 å»¶è¿Ÿç•¥æœ‰å¢åŠ 
- ä½†ä»åœ¨å¯æ¥å—èŒƒå›´å†… (< 100ms)

## ç»“è®º

{"âœ… **ç›®æ ‡è¾¾æˆ**: Hybrid Retrieval Recall@10 æå‡ " + f"{recall_10_improvement:.2f}% â‰¥ 15%" if target_met else "âŒ **ç›®æ ‡æœªè¾¾æˆ**: Recall@10 æå‡ " + f"{recall_10_improvement:.2f}% < 15%"}

## å†³ç­–å»ºè®®

{"**é‡‡çº³ Hybrid Retrieval æ–¹æ¡ˆ**" if target_met else "éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–:"}
{'' if target_met else '''
- è°ƒæ•´ Î± å‚æ•°
- å¢åŠ å›¾æ‰©å±•æ·±åº¦
- ä¼˜åŒ–å®ä½“æŠ½å–è´¨é‡
'''}

---
**å®éªŒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®é›†è§„æ¨¡**: {baseline.queries_count} queries, ~{baseline.queries_count * 5} memories
"""
    return report


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("PoC å®éªŒ: Hybrid vs Baseline Retrieval")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    random.seed(42)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("\nğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®é›†...")
    memories, queries = generate_test_dataset()
    print(f"   - è®°å¿†æ•°é‡: {len(memories)}")
    print(f"   - æŸ¥è¯¢æ•°é‡: {len(queries)}")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    print("\nğŸ”§ åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®åº“...")
    vector_db = SimulatedVectorDB(memories)
    graph_db = SimulatedGraphDB(memories)
    
    # è¿è¡Œ Baseline å®éªŒ
    print("\nğŸ”¬ è¿è¡Œ Baseline (Vector Only)...")
    baseline_result = run_experiment("baseline", queries, vector_db, graph_db)
    print(f"   - Recall@10: {baseline_result.recall_at_10:.4f}")
    print(f"   - MRR: {baseline_result.mrr:.4f}")
    
    # è¿è¡Œ Hybrid å®éªŒ
    print("\nğŸ”¬ è¿è¡Œ Hybrid (Vector + Graph)...")
    hybrid_result = run_experiment("hybrid", queries, vector_db, graph_db)
    print(f"   - Recall@10: {hybrid_result.recall_at_10:.4f}")
    print(f"   - MRR: {hybrid_result.mrr:.4f}")
    
    # è®¡ç®—æå‡
    improvement = ((hybrid_result.recall_at_10 - baseline_result.recall_at_10) / baseline_result.recall_at_10) * 100
    print(f"\nğŸ“ˆ Recall@10 æå‡: {improvement:+.2f}%")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆå®éªŒæŠ¥å‘Š...")
    report = generate_report(baseline_result, hybrid_result)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "poc_report.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"   æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "=" * 60)
    print(report)
    
    # è¿”å›æ˜¯å¦è¾¾æ ‡
    target_met = improvement >= 15
    print("=" * 60)
    if target_met:
        print("ğŸ‰ å®éªŒæˆåŠŸï¼Hybrid Retrieval è¾¾åˆ°é¢„æœŸç›®æ ‡ã€‚")
    else:
        print("âš ï¸ å®éªŒæœªè¾¾æ ‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
    print("=" * 60)
    
    return target_met


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
