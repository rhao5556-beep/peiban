"""
Generate human-readable LoCoMo evaluation report
"""
import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional


CATEGORY_NAMES = {
    1: "Factual Recall",
    2: "Temporal Understanding",
    3: "Reasoning & Inference",
    4: "Detailed Understanding",
}


def _load_json(path: Path) -> Any:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def generate_report(
    summary: Dict[str, Any],
    failures: Optional[List[Dict[str, Any]]] = None,
    lang: str = "en",
) -> str:
    """Generate markdown report"""
    
    is_zh = (lang or "").lower().startswith("zh")
    title = "LoCoMo è¯„æµ‹æŠ¥å‘Š" if is_zh else "LoCoMo Evaluation Report"
    overall_title = "æ€»ä½“è¡¨ç°" if is_zh else "Overall Performance"
    by_type_title = "æŒ‰é—®é¢˜ç±»å‹è¡¨ç°" if is_zh else "Performance by Question Type"
    by_task_title = "æŒ‰ä»»åŠ¡ç±»å‹è¡¨ç°" if is_zh else "Performance by Task Type"
    failure_title = "é”™è¯¯åˆ†æ" if is_zh else "Failure Analysis"
    insights_title = "æ´å¯Ÿä¸æ”¹è¿›å»ºè®®" if is_zh else "Insights & Recommendations"
    generated_label = "ç”Ÿæˆæ—¶é—´" if is_zh else "Generated"
    total_q_label = "é¢˜ç›®æ€»æ•°" if is_zh else "Total Questions"
    correct_label = "æ­£ç¡®æ•°" if is_zh else "Correct Answers"
    acc_label = "å‡†ç¡®ç‡" if is_zh else "Accuracy"
    em_acc_label = "ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡" if is_zh else "Exact Match Accuracy"
    conf_label = "å¹³å‡ç½®ä¿¡åº¦" if is_zh else "Average Confidence"
    method_label = "è¯„åˆ†æ–¹å¼" if is_zh else "Scoring Method"

    lines = [
        f"# {title}",
        "",
        f"**{generated_label}ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        f"## {overall_title}",
        "",
        f"- **{total_q_label}ï¼š** {summary.get('total', 0)}",
        f"- **{correct_label}ï¼š** {summary.get('correct', 0)}",
        f"- **{acc_label}ï¼š** {summary.get('accuracy', 0.0):.2%}",
        f"- **{em_acc_label}ï¼š** {summary.get('exact_match_accuracy', 0.0):.2%}",
        f"- **{conf_label}ï¼š** {summary.get('avg_confidence', 0.0):.2f}",
        f"- **{method_label}ï¼š** {summary.get('scoring_method', 'unknown')}",
        "",
    ]
    
    # Performance by category
    by_cat = summary.get("by_category", {})
    if by_cat:
        lines.extend([
            f"## {by_type_title}",
            "",
            "| Category | Type | Total | Correct | Accuracy | Exact Match | Confidence |" if not is_zh else "| ç±»åˆ« | ç±»å‹ | æ€»æ•° | æ­£ç¡® | å‡†ç¡®ç‡ | ç²¾ç¡®åŒ¹é… | ç½®ä¿¡åº¦ |",
            "|----------|------|-------|---------|----------|-------------|------------|",
        ])
        
        for cat_id in sorted(by_cat.keys()):
            cat_data = by_cat[cat_id]
            cat_name = cat_data.get("category_name", "Unknown")
            total = cat_data.get("total", 0)
            correct = cat_data.get("correct", 0)
            acc = cat_data.get("accuracy", 0.0)
            exact_acc = cat_data.get("exact_match_accuracy", 0.0)
            conf = cat_data.get("avg_confidence", 0.0)
            
            lines.append(
                f"| {cat_id} | {cat_name} | {total} | {correct} | {acc:.2%} | {exact_acc:.2%} | {conf:.2f} |"
            )
        
        lines.append("")
    
    # Category descriptions
    lines.extend([
        "### Question Type Descriptions" if not is_zh else "### é—®é¢˜ç±»å‹è¯´æ˜",
        "",
        "- **Category 1ï¼ˆäº‹å®å›å¿†ï¼‰ï¼š** å¯¹è¯å†å²ä¸­çš„ç›´æ¥äº‹å®" if is_zh else "- **Category 1 (Factual Recall):** Direct facts from conversation history",
        "- **Category 2ï¼ˆæ—¶é—´ç†è§£ï¼‰ï¼š** æ—¶é—´ç›¸å…³ä¿¡æ¯ä¸æ—¥æœŸ" if is_zh else "- **Category 2 (Temporal Understanding):** Time-related information and dates",
        "- **Category 3ï¼ˆæ¨ç†ä¸å½’çº³ï¼‰ï¼š** éœ€è¦è¶…è¶Šæ˜¾å¼äº‹å®çš„æ¨ç†" if is_zh else "- **Category 3 (Reasoning & Inference):** Requires reasoning beyond explicit facts",
        "- **Category 4ï¼ˆç»†èŠ‚ç†è§£ï¼‰ï¼š** å¯¹ä¸Šä¸‹æ–‡çš„ç»†èŠ‚æ€§ç†è§£" if is_zh else "- **Category 4 (Detailed Understanding):** Detailed comprehension of context",
        "",
    ])
    
    # Performance by task type
    by_task = summary.get("by_task_type", {})
    if by_task:
        lines.extend([
            f"## {by_task_title}",
            "",
            "| Task Type | Total | Correct | Accuracy | Exact Match | Confidence |" if not is_zh else "| ä»»åŠ¡ç±»å‹ | æ€»æ•° | æ­£ç¡® | å‡†ç¡®ç‡ | ç²¾ç¡®åŒ¹é… | ç½®ä¿¡åº¦ |",
            "|-----------|-------|---------|----------|-------------|------------|",
        ])
        
        for task_type in sorted(by_task.keys()):
            task_data = by_task[task_type]
            total = task_data.get("total", 0)
            correct = task_data.get("correct", 0)
            acc = task_data.get("accuracy", 0.0)
            exact_acc = task_data.get("exact_match_accuracy", 0.0)
            conf = task_data.get("avg_confidence", 0.0)
            
            lines.append(
                f"| {task_type} | {total} | {correct} | {acc:.2%} | {exact_acc:.2%} | {conf:.2f} |"
            )
        
        lines.append("")
    
    # Failure analysis
    if failures:
        lines.extend([
            f"## {failure_title}",
            "",
            f"**å¤±è´¥æ€»æ•°ï¼š** {len(failures)}" if is_zh else f"**Total Failures:** {len(failures)}",
            "",
        ])
        
        # Group failures by category
        failures_by_cat: Dict[str, List[Dict[str, Any]]] = {}
        for f in failures:
            cat = str(f.get("category", "unknown"))
            failures_by_cat.setdefault(cat, []).append(f)
        
        for cat_id in sorted(failures_by_cat.keys()):
            cat_name = CATEGORY_NAMES.get(int(cat_id), "Unknown") if cat_id.isdigit() else "Unknown"
            cat_failures = failures_by_cat[cat_id]
            
            lines.extend([
                f"### {cat_name} (Category {cat_id})",
                "",
                f"**Failures:** {len(cat_failures)}",
                "",
            ])
            
            # Show first 5 failures as examples
            for i, f in enumerate(cat_failures[:5]):
                lines.extend([
                    f"#### ç¤ºä¾‹ {i+1}ï¼ˆID: {f.get('id')}ï¼‰" if is_zh else f"#### Example {i+1} (ID: {f.get('id')})",
                    "",
                    f"**å‚è€ƒç­”æ¡ˆï¼š** {f.get('reference_answer', 'N/A')}" if is_zh else f"**Reference:** {f.get('reference_answer', 'N/A')}",
                    "",
                    f"**ç³»ç»Ÿå›ç­”ï¼š** {f.get('model_answer', 'N/A')}" if is_zh else f"**Model Answer:** {f.get('model_answer', 'N/A')}",
                    "",
                ])
                
                if "reasoning" in f:
                    lines.extend([
                        f"**Judge ç†ç”±ï¼š** {f.get('reasoning', 'N/A')}" if is_zh else f"**Judge Reasoning:** {f.get('reasoning', 'N/A')}",
                        "",
                    ])
                
                if "exact_match" in f:
                    lines.extend([
                        f"**ç²¾ç¡®åŒ¹é…ï¼š** {'æ˜¯' if f.get('exact_match') else 'å¦'}" if is_zh else f"**Exact Match:** {'Yes' if f.get('exact_match') else 'No'}",
                        "",
                    ])
            
            if len(cat_failures) > 5:
                lines.append(f"*... and {len(cat_failures) - 5} more failures in this category*")
                lines.append("")
    
    # Insights and recommendations
    lines.extend([
        f"## {insights_title}",
        "",
    ])
    
    # Analyze performance
    overall_acc = summary.get("accuracy", 0.0)
    exact_match_acc = summary.get("exact_match_accuracy", 0.0)
    
    if overall_acc >= 0.8:
        lines.append("âœ… **è¡¨ç°ä¼˜ç§€ï¼š** ç³»ç»Ÿå±•ç¤ºå‡ºè¾ƒå¼ºçš„é•¿æœŸè®°å¿†èƒ½åŠ›ã€‚" if is_zh else "âœ… **Excellent Performance:** The system demonstrates strong long-term memory capabilities.")
    elif overall_acc >= 0.6:
        lines.append("âš ï¸ **è¡¨ç°è‰¯å¥½ï¼š** ç³»ç»Ÿè®°å¿†èƒ½åŠ›å°šå¯ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚" if is_zh else "âš ï¸ **Good Performance:** The system shows decent memory but has room for improvement.")
    else:
        lines.append("âŒ **éœ€è¦æ”¹è¿›ï¼š** ç³»ç»Ÿåœ¨é•¿æœŸè®°å¿†ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¼±ã€‚" if is_zh else "âŒ **Needs Improvement:** The system struggles with long-term memory tasks.")
    
    lines.append("")
    
    # LLM vs exact match gap
    if overall_acc > exact_match_acc + 0.05:
        gap = overall_acc - exact_match_acc
        lines.extend([
            f"ğŸ“Š **LLM Judge ä¼˜åŠ¿ï¼š** LLM è¯„åˆ†æ¯”ç²¾ç¡®åŒ¹é…å¤šè¯†åˆ«å‡º {gap:.1%} çš„æ­£ç¡®ç­”æ¡ˆï¼Œè¯´æ˜ç³»ç»Ÿå­˜åœ¨â€œè¯­ä¹‰æ­£ç¡®ä½†æªè¾ä¸åŒâ€çš„å›ç­”ã€‚" if is_zh else f"ğŸ“Š **LLM Judge Benefit:** LLM scoring found {gap:.1%} more correct answers than exact match, ",
            "" if is_zh else "indicating the system produces semantically correct answers that differ in phrasing.",
            "",
        ])
    
    # Category-specific insights
    if by_cat:
        weakest_cat = min(by_cat.items(), key=lambda x: x[1].get("accuracy", 0.0))
        strongest_cat = max(by_cat.items(), key=lambda x: x[1].get("accuracy", 0.0))
        
        weak_name = weakest_cat[1].get("category_name", "Unknown")
        strong_name = strongest_cat[1].get("category_name", "Unknown")
        weak_acc = weakest_cat[1].get("accuracy", 0.0)
        strong_acc = strongest_cat[1].get("accuracy", 0.0)
        
        lines.extend([
            f"ğŸ¯ **æœ€å¼ºé¡¹ï¼š** {strong_name}ï¼ˆå‡†ç¡®ç‡ {strong_acc:.1%}ï¼‰" if is_zh else f"ğŸ¯ **Strongest Area:** {strong_name} ({strong_acc:.1%} accuracy)",
            f"ğŸ”§ **æœ€å¼±é¡¹ï¼š** {weak_name}ï¼ˆå‡†ç¡®ç‡ {weak_acc:.1%}ï¼‰" if is_zh else f"ğŸ”§ **Needs Work:** {weak_name} ({weak_acc:.1%} accuracy)",
            "",
        ])
    
    lines.extend([
        "### Recommendations" if not is_zh else "### å»ºè®®",
        "",
    ])
    
    # Specific recommendations based on category performance
    if by_cat:
        for cat_id, cat_data in by_cat.items():
            cat_name = cat_data.get("category_name", "Unknown")
            acc = cat_data.get("accuracy", 0.0)
            
            if acc < 0.6:
                if cat_id == "1":
                    lines.append("- **Factual Recall:** Improve entity extraction and graph storage reliability")
                elif cat_id == "2":
                    lines.append("- **Temporal Understanding:** Enhance temporal entity recognition and date normalization")
                elif cat_id == "3":
                    lines.append("- **Reasoning:** Strengthen multi-hop retrieval and inference capabilities")
                elif cat_id == "4":
                    lines.append("- **Detailed Understanding:** Improve context preservation and detail retention")
    
    lines.extend([
        "",
        "---",
        "",
        "*æœ¬æŠ¥å‘Šç”± LoCoMo è¯„æµ‹æµæ°´çº¿è‡ªåŠ¨ç”Ÿæˆã€‚*" if is_zh else "*This report was generated automatically by the LoCoMo evaluation pipeline.*",
    ])
    
    return "\n".join(lines)


def main() -> None:
    p = argparse.ArgumentParser(description="Generate LoCoMo evaluation report")
    p.add_argument("--summary_path", required=True, help="Path to scoring summary JSON")
    p.add_argument("--failures_path", default="", help="Path to failures JSON")
    p.add_argument("--output_path", required=True, help="Path to save report")
    p.add_argument("--lang", choices=["en", "zh"], default="en", help="Report language")
    args = p.parse_args()
    
    summary = _load_json(Path(args.summary_path))
    
    failures = None
    if args.failures_path:
        failures_path = Path(args.failures_path)
        if failures_path.exists():
            failures = _load_json(failures_path)
    
    report = generate_report(summary, failures, lang=args.lang)
    
    output_path = Path(args.output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(report, encoding="utf-8")
    
    print(f"Report generated: {output_path}")
    print("\n" + "="*60)
    print(report)


if __name__ == "__main__":
    main()
